{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This notebook preprocesses the data to have correct time and treatment metadata."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pathlib\n",
                "from pprint import pprint\n",
                "\n",
                "import pandas as pd\n",
                "import pyarrow.parquet as pq"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_subset = True\n",
                "samples_per_group = 100\n",
                "\n",
                "# path to the data\n",
                "feature_selected_profiles_data_dir = pathlib.Path(\n",
                "    \"../../4.processing_profiled_features/data/feature_selected_data\"\n",
                ").resolve(strict=True)\n",
                "list_of_files = list(feature_selected_profiles_data_dir.glob(\"*.parquet\"))\n",
                "\n",
                "input_data_dict = {\n",
                "    \"first_time\": {\n",
                "        \"input_file_path\": list_of_files[0],\n",
                "        \"output_data_dir\": pathlib.Path(\"../data/first_time\").resolve(),\n",
                "        \"figure_dir\": pathlib.Path(\"../figures/first_time\").resolve(),\n",
                "    },\n",
                "    \"within_time\": {\n",
                "        \"input_file_path\": list_of_files[1],\n",
                "        \"output_data_dir\": pathlib.Path(\"../data/within_time\").resolve(),\n",
                "        \"figure_dir\": pathlib.Path(\"../figures/within_time\").resolve(),\n",
                "    },\n",
                "    \"pan_time\": {\n",
                "        \"input_file_path\": list_of_files[2],\n",
                "        \"output_data_dir\": pathlib.Path(\"../data/pan_time\").resolve(),\n",
                "        \"figure_dir\": pathlib.Path(\"../figures/pan_time\").resolve(),\n",
                "    },\n",
                "}\n",
                "pprint(input_data_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for dataset in input_data_dict:\n",
                "    input_data_dict[dataset][\"output_data_dir\"].mkdir(parents=True, exist_ok=True)\n",
                "    input_data_dict[dataset][\"figure_dir\"].mkdir(parents=True, exist_ok=True)\n",
                "    if data_subset:\n",
                "        subset_data_output_file_path = pathlib.Path(\n",
                "            input_data_dict[dataset][\"output_data_dir\"]\n",
                "            / f'{input_data_dict[dataset][\"input_file_path\"].stem}_subset_testing_data.parquet'\n",
                "        ).resolve()\n",
                "        data = pd.read_parquet(\n",
                "            input_data_dict[dataset][\"input_file_path\"], columns=[\"Metadata_Well\"]\n",
                "        )\n",
                "        data_df = data.groupby(\"Metadata_Well\").apply(\n",
                "            lambda x: x.sample(min(len(x), samples_per_group), random_state=0)\n",
                "        )\n",
                "\n",
                "        # get the indexes of the data\n",
                "        data_idx = data.index\n",
                "        data = pd.concat(\n",
                "            [\n",
                "                pd.read_parquet(\n",
                "                    input_data_dict[dataset][\"input_file_path\"], columns=[col]\n",
                "                ).iloc[data_idx]\n",
                "                for col in pq.read_schema(\n",
                "                    input_data_dict[dataset][\"input_file_path\"]\n",
                "                ).names\n",
                "            ],\n",
                "            axis=1,\n",
                "        )\n",
                "        # save the subset data\n",
                "        data.to_parquet(subset_data_output_file_path)\n",
                "        data.head()\n",
                "    else:\n",
                "        data = pd.read_parquet(input_data_dict[dataset][\"input_file_path\"])\n",
                "        data.head()\n",
                "\n",
                "    # perform preprocessing on each data\n",
                "    # sort the time and replace with 1, 2, 3, 4\n",
                "    time_mapping = {\n",
                "        time: i for i, time in enumerate(data[\"Metadata_Plate\"].sort_values().unique())\n",
                "    }\n",
                "    # check if the new columns exist, if so drop them\n",
                "    if \"Metadata_treatment_serum\" in data.columns:\n",
                "        data.drop(columns=[\"Metadata_treatment_serum\"], inplace=True)\n",
                "    if \"Metadata_Time\" in data.columns:\n",
                "        data.drop(columns=[\"Metadata_Time\"], inplace=True)\n",
                "    # Combine all new columns at once to avoid fragmentation\n",
                "    new_columns = pd.DataFrame(\n",
                "        {\n",
                "            \"Metadata_treatment_serum\": data[\"Metadata_treatment\"]\n",
                "            + \" \"\n",
                "            + data[\"Metadata_serum\"],\n",
                "            \"Metadata_Time\": data[\"Metadata_Plate\"].map(time_mapping),\n",
                "        }\n",
                "    )\n",
                "    data = pd.concat([data, new_columns], axis=1)\n",
                "\n",
                "    if data_subset:\n",
                "        data.to_parquet(subset_data_output_file_path)\n",
                "    else:\n",
                "        # over write the current parquet file\n",
                "        data.to_parquet(input_data_dict[dataset][\"input_file_path\"])\n",
                "\n",
                "    print(f\"Preprocessed data for {dataset} has the shape: {data.shape}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pyroptosis_timnelapse_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
